---
title: "Data set building"
author: "Alain Lesaffre"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---
```{r  init, echo=FALSE, message=FALSE, warning=FALSE}
options( scipen = 99999)
library(tidyverse)
library(dplyr)
library(vtreat)
library(DataExplorer)
library(ggplot2)
library(see)
library(ggshadow)
library(readr)
library(readxl)
library(performance)
library(caret)
library(ranger)
library(brglm)
source("./common.R")
c4c_data   <-janitor::clean_names(readxl::read_xlsx(c4c_file ))
bci_data   <-janitor::clean_names(readxl::read_xlsx(dulux_raw_file, sheet="BCI Lead data" ) )
opportunity_data <-janitor::clean_names( read.csv(opportunity_file )) %>%
                            dplyr::mutate( date = stringr::str_sub( created_on, 1L, 19L),
                                           date = stringr::str_replace( date ,"T", " "),
                                           date =as.POSIXct(date)) %>% 
                            dplyr::mutate( opportunity = 1) %>% 
                            dplyr::select( btd_reference_id, date, opportunity)


op_c4c_data  <- c4c_data   %>% 
                        dplyr::inner_join( opportunity_data, by=c( "id" = "btd_reference_id")) 
starting_date  <-min(op_c4c_data$date)

noop_c4c_data  <-c4c_data  %>%
                       dplyr::mutate( date = stringr::str_sub( creation_date_time, 1L, 19L),
                                      date = as.POSIXct(date) )%>%
                       dplyr::filter( !(id %in% op_c4c_data$id)) %>%
                       dplyr::mutate( opportunity = 0)


to_remove_from_op_c4c <-names(op_c4c_data)[!(names(op_c4c_data) %in% names(noop_c4c_data))]
op_c4c_data           <- op_c4c_data %>%
                                 select(-to_remove_from_op_c4c)
clean_c4c_data <- dplyr::bind_rows( noop_c4c_data,
                                 op_c4c_data  )


all_data   <-bci_data  %>%
                      dplyr::inner_join(clean_c4c_data, by=c( "projectid" = "external_id")) %>% 
                      dplyr::select("projectid","name", "company", "qualification_level_code_text", "user_status_code_text" ,"account_life_cycle_status_code_text",
                                    "priority_code_text",  "group_code_text", "account_state", "consistency_status_code_text",
                                    "project_type", "projectstage","project_status" ,"remarks", "origin_type_code_text", "creation_date_time",
                                    "life_cycle_status_code_text", "value", "contact_uuid","opportunity") %>%
                      dplyr::mutate( creation_date_time = stringr::str_sub(creation_date_time, 1,19),
                                     creation_date = as.POSIXct(creation_date_time),
                                     beg_date      = as.Date(creation_date)) %>%
                      dplyr::mutate( log_value = log(value)) %>%
                      dplyr::select(-creation_date_time, - contact_uuid ) 



lubridate::day(all_data$beg_date) <-1
write.csv(all_data, "../data/data_set.csv")

#
#
#  As in EDA
#
#
all_data <-all_data %>% 
             dplyr::mutate( project_type = stringr::str_to_lower(project_type),
                            type = ifelse( stringr::str_detect(project_type, "apartment"),
                                           "apartment", 
                                           NA),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "townhouse"),
                                           "townhouse", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "house|chalet"),
                                           "house", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "commercial|motel|hotel|showroom|industrial|supermarket|factory"),
                                           "commercial", 
                                           type),
                             type = ifelse( is.na(type) & stringr::str_detect(project_type, "office"),
                                           "commercial", 
                                           type),
                             type = ifelse( is.na(type) & stringr::str_detect(project_type, "road"),
                                           "road", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "streetscap|intersection"),
                                           "road", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "aged care|retirement"),
                                           "aged_care", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "retirement village|senior living villa|senior living unit"),
                                           "aged_care", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "cycle"),
                                           "road", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "hotel"),
                                           "hospitality", 
                                           type),
                             type = ifelse( is.na(type) & stringr::str_detect(project_type, "restaurant"),
                                           "hospitality", 
                                           type),
                            type = ifelse( is.na(type) & stringr::str_detect(project_type, "prison|school|classroom|community centre|social housing|bridge|childcare centre|hospital|medical|water infrastructure|childcare centre|airport"),
                                           "public", 
                                           type),
                            type  = ifelse( is.na(type), 
                                            "other",
                                            type)
                            )
all_data <-all_data %>% 
             dplyr::mutate( project_status =ifelse( stringr::str_detect(project_status, "Rezoning|Tenders To Be Called|Site Works Commenced|Construction Certificate Approved|Development Approval|Tenders Called"), "big_effect",project_status ))
saveRDS(all_data, second_data)
```
# Summary

In this document, we build a new data set compare to the one used in **EDA**. The point is to add mre fields, the response we are working with is the opportunities, which is this case is very different that in **EDA** for Warm but similar for Hot. 

To reach good prediction we transform the data using conditional probability based on the effect. What is a surprise is thae have clear speartion between **Hot** and **Warm** and that for stability it is better to pick up a tree method. 

# Overview

The data set has been built based on the date of the first oppurtunity, all observation before this sdate has been removed. One could notice that the field user_status_code_text with **Converted** the meaning is not a transformation in opportunity. 

The data set information are the following:

|  Attribute           | Value     |
|----------------------|-----------|
| Starting Date        | `r as.character(starting_date)` |
| Number observations  | `r nrow(all_data)` | 
| Number of hot        | `r sum( all_data$qualification_level_code_text =="Hot")`|
| Number of warm        | `r sum( all_data$qualification_level_code_text =="Warm")`|
| Median value hot     | `r median( all_data$value[all_data$qualification_level_code_text =="Hot"])`|
| Opportunities        | `r sum(all_data$opportunity)` |
| Hot Opportunities    | `r sum(all_data$opportunity[all_data$qualification_level_code_text =="Hot"], na.rm = TRUE)`|
| Warm Opportunities   | `r sum(all_data$opportunity[all_data$qualification_level_code_text =="Warm"], na.rm = TRUE)` |
| Number converted     | `r sum(all_data$user_status_code_text =="Converted")` |
| Number Hot converted | `r sum(all_data$user_status_code_text[all_data$qualification_level_code_text =="Hot"] =="Converted")`|
| Number Warm converted | `r sum(all_data$user_status_code_text[all_data$qualification_level_code_text =="Warm"] =="Converted")`|
| Percent Hot converted  | `r round(sum(all_data$user_status_code_text[all_data$qualification_level_code_text =="Hot"] =="Converted")/ sum(all_data$qualification_level_code_text =="Hot", na.rm = TRUE),4)* 100`|
| Percent Warm converted | `r round(sum(all_data$user_status_code_text[all_data$qualification_level_code_text =="Warm"] =="Converted")/ sum(all_data$qualification_level_code_text =="Warm", na.rm = TRUE),4)* 100`|
| Percent Hot opportunity  | `r round(sum(all_data$opportunity[all_data$qualification_level_code_text =="Hot"])/ sum(all_data$qualification_level_code_text =="Hot", na.rm = TRUE),4)* 100`|
| Percent Warm opportunity  | `r round(sum(all_data$opportunity[all_data$qualification_level_code_text =="Warm"])/ sum(all_data$qualification_level_code_text =="Warm", na.rm = TRUE),4)* 100`|


The project type after transformation are distributed as follow, the other could be further transformed:

```{r type_porject,echo=FALSE}
knitr::kable(table(all_data$type), col.names = c("Type","Number occurences"))
```


The value of the opportunities based on the qualification Hot or Warm is as follow.

```{r value_op, echo=FALSE, fig.align="center"}
ggplot( data = all_data, aes(x=log_value))+
        geom_density(aes(group=qualification_level_code_text, fill=qualification_level_code_text, col=qualification_level_code_text), alpha=.8) +
        # geom_rug()+
        theme_modern()+
        labs(title="Values / Oppotunity",
             x="log value",
             y="Frequency")

```

## Data preparation 
```{r vtreat, warning =FALSE,echo=FALSE}
variables_to_process <-c("type","log_value", "account_state", "project_status", "projectstage", "consistency_status_code_text", "life_cycle_status_code_text")


warm_data   <- all_data %>%
                   dplyr::filter( qualification_level_code_text =="Warm")
plan_data <-vtreat::designTreatmentsC(
        warm_data,
        varlist = variables_to_process,
        outcomename = "opportunity",
        outcometarget =  1,
        verbose  = FALSE
)
warm_to_process <-vtreat::prepare(plan_data ,
                                  warm_data)
```

Due to the fact that we deal with quantitative variables, we do further transformation such as mapping the effect of the categories. This transformation adds a level of complexity concerning the interpretation as we deal with conditional probability. As we shall see below the results one **Warm** are acceptable. 

# Time factor 
```{r time, echo=FALSE}
cycle_vic <- all_data  %>%
                   dplyr::filter( account_state =="VIC") %>%
                   dplyr::group_by(beg_date) %>%
                   dplyr::summarise(nb_opportunity = sum( opportunity ==1),
                                    lost_opportunity = sum( opportunity == 0))
                   

```

Construction is cyclic, in the following garphic we show the opportunity for Victoria.

```{r vic_dsp, echo=FALSE,fig.align="center"}
ggplot( data = cycle_vic)+
  geom_line( aes( x= beg_date, y= nb_opportunity, group=1), col="orange", size=1) +
  theme_modern()

```





# Simple models 
```{r simple_models, echo=FALSE, warning= FALSE}
warm_model <-glm( opportunity  ~ .,
                  family = binomial,
                  data = warm_to_process %>% select( !which(stringr::str_detect(names(warm_to_process), "catP$"))))
warm_predict <-predict(warm_model  , type = "response")
logistics_deviance <-sum(-2*log(abs(warm_to_process$opportunity -warm_predict)))
deviance_warm <- residuals(warm_model , type = "response")
to_check <-as.data.frame.matrix( summary(warm_model)$coefficients)
```

As we shall see with **Hot** as well, we have do not have significant coefficient in linear model but good prediction, which  are confirmed using a tree ensemble. What does this mean really? 


The results for **Warm** are as follow with cut off of .5 and the **catB** only, the transformation in this case is totally based on effect:

| Reference       | Value  |
|-----------------|--------|
|  True positive  | `r sum( warm_to_process$opportunity ==1 & warm_predict > .5)` |
|  False positive | `r sum( warm_to_process$opportunity ==0 & warm_predict > .5)` |
|  False negative | `r sum( warm_to_process$opportunity ==1 & warm_predict <= .5)` |
|  True negative  | `r sum( warm_to_process$opportunity ==0  & warm_predict <= .5)` |

The *working* residual is very small with such a prediction, the total residual is of `r sum(deviance_warm)`.


Some variables used are the conditional probabilities expresses as *opportunity | category variables*. 

We should not forget that we have  converted the **type**, **state** and **project state** with conditional probability. Using type of project, which is a transformed data, the **other** category has a probability of .33 to be a non opportunity, or **road** with have 0.065 to be a non opportunity as shown below the apartment has a value of .15 as in hot this value of .55.  


Most significant variables:

1. log of the value 
2. The type transformed  (Conditional probability) 
3. The hot encoding of the type (Transform data as shown above)
4. The life_cycle_status_code (Conditional probability)
5. The hot encoding life_cycle_status_code (Not transformed) 


## Hot Model
```{r hotmooel, echo=FALSE, warning=FALSE}
variables_to_process <-c("type","log_value", "account_state", "project_status", "projectstage", "consistency_status_code_text", "life_cycle_status_code_text")


hot_data   <- all_data %>%
                   dplyr::filter( qualification_level_code_text =="Hot")
plan_data <-vtreat::designTreatmentsC(
        hot_data,
        varlist = variables_to_process,
        outcomename = "opportunity",
        outcometarget =  1,
        verbose  = FALSE
)
hot_to_process <-vtreat::prepare(plan_data ,
                                  hot_data)

hot_model <-glm( opportunity  ~ .,
                  family = binomial,
                  data = hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))))

hot_predict <-predict(hot_model  , type = "response")
deviance_hot <- residuals(hot_model , type = "response")
```

We build a global **Hot** model similar to to **Warm**, the only difference is the size of the data set, which is of `r nrow(hot_data)` observations. 


The results for **Hot** are as follow with cut off of .5 and the **catB** only, the transformation in this case is totally based on effect:

| Reference       | Value  |
|-----------------|--------|
|  True positive  | `r sum( hot_to_process$opportunity ==1 & hot_predict > .5)` |
|  False positive | `r sum( hot_to_process$opportunity ==0 & hot_predict > .5)` |
|  False negative | `r sum( hot_to_process$opportunity ==1 & hot_predict <= .5)` |
|  True negative  | `r sum( hot_to_process$opportunity ==0  & hot_predict <= .5)` |

The *working* residual is very small with such a prediction, the total residual is of `r sum(deviance_hot)`. And the most significant value is the log of the value based on the Z value.  

Most significant variable:

1. log of the value 
2. The life_cycle_status_code (Conditional probability)
3. The life_cycle_status_code  hot encoding Converted and Qualified 

The R2 is of `r r2(hot_model)`, which is *good*.

A more detail view of the model with parameters, it is not optimum as the sd includes zero most of the time. The main question is how the model will behave based on new information. As the sd is high, too high.  


```{r dsp_coef_model, echo=FALSE}
modelsummary::modelsummary(hot_model)
```

To check we show the prediction, we notice that two group could be well separated and therefor exact logistic could be used. Th results will not be very different therefor we use a tree method which give very similar results but is more stable. 

```{r prediction_hot, echo=FALSE, fig.align="center"}
hist(hot_predict, col="orange", main="prediction hot", breaks=20)
```

### Firth method 

As mentioned we have instability in the logistic regression, therefore we use the reduction of **Firth**. The results are show below and are slightly better than with tree. 

```{r first_method, echo=FALSE, warning=FALSE}
hot_firth <- brglm( opportunity  ~ .,  data = hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))), pl= TRUE)
predict_firth <-predict(hot_firth, type="response")
predict_firth_fact <-factor(ifelse( predict_firth > .5, 1,0))
hot_firth_confusion <-confusionMatrix(predict_firth_fact, as.factor(hot_to_process$opportunity))
fourfoldplot(as.array(hot_firth_confusion$table), color = c("lightblue", "orange"))
```

The main coefficient with impact and significance as expected the conditional probability are part of the variable with significance. The **catB** of the project state has quite a high coefficient specially associated with the state **Deferred** as one could notice. The surprise is more the type **apartement** and **commercial** in **Catb** the conditional probability is of about .5 

```{r firsth_coef, echo=FALSE}
all_sum <-summary(hot_firth )
names_coef <-rownames(all_sum$coefficients) [which(all_sum$coefficients[,4] <.05)]
value_coeff  <-all_sum$coefficients[,1][which(all_sum$coefficients[,4] <.05)]
dps_coef <-data.frame( Variable = names_coef ,
                       Coef_Value = value_coeff )
knitr::kable(value_coeff )
```


## Build tree 

```{r ranger_tree, echo=FALSE}
library(ranger)
ranger_model_hot <-ranger(  opportunity  ~ .,
                            data = hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))))
hot_ranger_predict <-predict(ranger_model_hot, data= hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))))
```

As mentioned we now build a tree to avoid the instability. The results are shown below, there is nearly no difference between the linear model and the tree type of model, apart from a small change in false positive. For simplicity we could stay with the linear model. The next step will be to use cross valiadtion to check the stability of the model.  



| Reference       | Value  |
|-----------------|--------|
|  True positive  | `r sum( hot_to_process$opportunity ==1 & hot_ranger_predict$predictions > .5)` |
|  False positive | `r sum( hot_to_process$opportunity ==0 & hot_ranger_predict$predictions > .5)` |
|  False negative | `r sum( hot_to_process$opportunity ==1 & hot_ranger_predict$predictions <= .5)` |
|  True negative  | `r sum( hot_to_process$opportunity ==0  & hot_ranger_predict$predictions <= .5)` |


Out of the false positives, it could be of interest to check what are those observations? Are they exceptional? The confusion matrix is shown below.

```{r cross_validation, echo=FALSE, message=FALSE, fig.align="center"}
hot_control <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10)

rf_caret_hot <- train(as.factor(opportunity) ~ ., 
                data =hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))), 
                method = "ranger",
                trControl = hot_control)

rf_cater_res <-predict(rf_caret_hot, data = hot_to_process %>% select( !which(stringr::str_detect(names(hot_to_process), "catP$"))))
hot_confusion <-confusionMatrix(rf_cater_res, as.factor(hot_to_process$opportunity))
 fourfoldplot(as.array(hot_confusion$table), color = c("lightblue", "orange"))
```
The statistics for the model are as follow, on the whole the model is good.

```{r dsphot_conf, echo=FALSE}
all_stats <-round(hot_confusion$byClass,2)
dsp_stats <-data.frame( Statistics =attributes(all_stats)$names,
                        values = all_stats )
rownames(dsp_stats) <-NULL
knitr::kable(dsp_stats)

```

## Some details about the preprocessing

The preprocessing is made by modeling the inferences. The **CatB** is the contribution to non informative level, this is an impact variable therefor for type the impact is:
$$ Impact(type) = E[type|Category] - E[type]$$
## Few notes

In both cases the logistic regression is nearlt perfact and the two groups could linearly separated, which means that first the logistic regression is not satbble ans then a treeem method is more adequate. 






